{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776905b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a8318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Basic Config (only modify this) =====================\n",
    "# Do not include the .csv suffix\n",
    "TARGET_FILE = 'event_7-abortion_finger'   # <<< Change this to your file prefix\n",
    "CSV_PATH = TARGET_FILE + '.csv'\n",
    "# ===========================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db652a",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d99d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "Data = pd.read_csv(CSV_PATH)\n",
    "print('Raw shape:', Data.shape)\n",
    "\n",
    "# Count active/inactive samples (last column is label)\n",
    "num_active = (Data.iloc[:, -1] != 0).sum()\n",
    "num_inactive = (Data.iloc[:, -1] == 0).sum()\n",
    "print('number of active: ', num_active)\n",
    "print('number of inactive: ', num_inactive)\n",
    "\n",
    "# Drop columns containing '#NAME?'\n",
    "mask_bad = Data.apply(lambda c: (c == '#NAME?').any())\n",
    "bad_cols = Data.columns[mask_bad].tolist()\n",
    "if bad_cols:\n",
    "    print('Drop columns containing #NAME?:', bad_cols)\n",
    "    Data = Data.drop(columns=bad_cols)\n",
    "\n",
    "# Convert all possible columns to numeric, others to NaN\n",
    "Data_num = Data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop all-NaN columns\n",
    "all_nan_cols = [c for c in Data_num.columns if Data_num[c].isna().all()]\n",
    "if all_nan_cols:\n",
    "    print('Drop all-NaN columns:', all_nan_cols)\n",
    "    Data_num = Data_num.drop(columns=all_nan_cols)\n",
    "\n",
    "# Drop columns containing any NaN\n",
    "na_cols = Data_num.columns[Data_num.isna().any()].tolist()\n",
    "if na_cols:\n",
    "    print('Drop columns containing NaN:', na_cols)\n",
    "    Data_num = Data_num.drop(columns=na_cols)\n",
    "\n",
    "# Split features and labels (keep column 1:-1 as X, last column as y)\n",
    "# If the first column is not an ID, change [1:-1] to [: -1]\n",
    "X = Data_num.iloc[:, 1:-1].copy()\n",
    "y = Data_num.iloc[:, -1].copy()\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)\n",
    "print('y unique:', y.unique())\n",
    "\n",
    "# Drop columns with extreme values (>= 10000)\n",
    "extreme_cols = [c for c in X.columns if pd.to_numeric(X[c], errors='coerce').max() >= 10000]\n",
    "if extreme_cols:\n",
    "    print('Drop extreme columns (>=10000):', extreme_cols)\n",
    "    X = X.drop(columns=extreme_cols)\n",
    "\n",
    "print('X shape after extreme filter:', X.shape)\n",
    "\n",
    "# Variance threshold (remove features with 0 variance)\n",
    "selector = VarianceThreshold()\n",
    "X_var0 = selector.fit_transform(X)\n",
    "\n",
    "# Get feature names after VarianceThreshold\n",
    "try:\n",
    "    list4 = selector.get_feature_names_out(input_features=X.columns)\n",
    "except TypeError:\n",
    "    support_idx = selector.get_support(indices=True)\n",
    "    list4 = X.columns[support_idx].to_list()\n",
    "\n",
    "X_var0 = pd.DataFrame(X_var0, columns=list4)\n",
    "print('After VarianceThreshold:', X_var0.shape)\n",
    "\n",
    "# Mutual information feature selection\n",
    "mi = mutual_info_classif(X_var0, y)\n",
    "k = int((mi > 0).sum())\n",
    "k = max(k, 1)  # Keep at least one feature\n",
    "mlc = SelectKBest(mutual_info_classif, k=k).fit(X_var0, y)\n",
    "X_fsmic = mlc.transform(X_var0)\n",
    "\n",
    "# Cross-validation using RandomForest\n",
    "cr = cross_val_score(\n",
    "    RandomForestClassifier(n_estimators=10, random_state=0),\n",
    "    X_fsmic, y, cv=5\n",
    ").mean()\n",
    "\n",
    "print('k (mi>0):', k)\n",
    "print('CV mean score:', cr)\n",
    "print('X_fsmic shape:', X_fsmic.shape)\n",
    "\n",
    "# Get selected feature names\n",
    "try:\n",
    "    list5 = mlc.get_feature_names_out(input_features=X_var0.columns)\n",
    "except TypeError:\n",
    "    support_idx2 = mlc.get_support(indices=True)\n",
    "    list5 = X_var0.columns[support_idx2].to_list()\n",
    "\n",
    "X_fsmic = pd.DataFrame(X_fsmic, columns=list5)\n",
    "\n",
    "# Save selected features and labels\n",
    "X_fsmic.to_csv(TARGET_FILE + '-2.csv')\n",
    "X_ = pd.read_csv(TARGET_FILE + '-2.csv', index_col=0)\n",
    "\n",
    "y.to_csv(TARGET_FILE + '-3.csv')\n",
    "y_ = pd.read_csv(TARGET_FILE + '-3.csv', index_col=0).values.ravel()\n",
    "\n",
    "print('X_ shape:', X_.shape, '| y_ shape:', y_.shape)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X_)\n",
    "X_std = pd.DataFrame(X_std, columns=X_.columns)\n",
    "\n",
    "# Train/test split\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(\n",
    "    X_std, y_, test_size=0.2, random_state=0, stratify=y_\n",
    ")\n",
    "\n",
    "print('Xtrain/Xtest/Ytrain/Ytest shapes:',\n",
    "      Xtrain.shape, Xtest.shape, (len(Ytrain),), (len(Ytest),))\n",
    "\n",
    "# Show class distribution\n",
    "Ytrain = pd.Series(Ytrain)\n",
    "Ytest = pd.Series(Ytest)\n",
    "print('Train class counts:\\n', Ytrain.value_counts())\n",
    "print('Test class counts:\\n', Ytest.value_counts())\n",
    "\n",
    "# Apply SMOTE on training data\n",
    "sm = SMOTE(random_state=42)\n",
    "Xtrain_, Ytrain_ = sm.fit_resample(Xtrain, Ytrain)\n",
    "\n",
    "print('After SMOTE n_samples:', Xtrain_.shape[0])\n",
    "print('Resampled train class counts:\\n', pd.Series(Ytrain_).value_counts())\n",
    "\n",
    "# Save data splits\n",
    "Xtrain.to_csv(TARGET_FILE + '-Xtrain.csv')\n",
    "Xtrain_.to_csv(TARGET_FILE + '-Xtrain_.csv')\n",
    "Xtest.to_csv(TARGET_FILE + '-Xtest.csv')\n",
    "Ytrain.to_csv(TARGET_FILE + '-Ytrain.csv')\n",
    "pd.Series(Ytrain_).to_csv(TARGET_FILE + '-Ytrain_.csv', index=False, header=True)\n",
    "Ytest.to_csv(TARGET_FILE + '-Ytest.csv')\n",
    "\n",
    "print('All done âœ”')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
