{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f6b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb0439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== BASIC CONFIG (EDIT THIS ONLY) =====================\n",
    "DATA_PREFIX = \"event_7\"  # <<< change to your file prefix (no .csv)\n",
    "# ========================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a2ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Utilities -------------------- #\n",
    "def safe_index_col(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Safely read CSV with the first column as index (covers 'Unnamed: 0' cases).\n",
    "    \"\"\"\n",
    "    return pd.read_csv(path, index_col=0)\n",
    "\n",
    "\n",
    "def get_scores(model, X: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return a continuous score for AUC metrics:\n",
    "    - Try predict_proba[:, 1]\n",
    "    - Else try decision_function\n",
    "    - Else fall back to predict (labels) â€” not ideal for AUC, but prevents crashes\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        try:\n",
    "            proba = model.predict_proba(X)\n",
    "            if proba.shape[1] == 2:\n",
    "                return proba[:, 1]\n",
    "        except Exception:\n",
    "            pass\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        try:\n",
    "            scores = model.decision_function(X)\n",
    "            # If decision_function returns shape (n_samples, 2), take positive class\n",
    "            if scores.ndim == 2 and scores.shape[1] == 2:\n",
    "                return scores[:, 1]\n",
    "            return scores\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Fallback: predicted labels (AUC will be less informative)\n",
    "    return model.predict(X)\n",
    "\n",
    "\n",
    "def evaluate_split(y_true: pd.Series, y_pred: np.ndarray, y_scores: Optional[np.ndarray]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compute confusion matrix-based metrics and AUC metrics from scores when available.\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    pre = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    # AUC metrics: require both classes present and a continuous score\n",
    "    roc = None\n",
    "    pr_auc = None\n",
    "    if y_scores is not None and len(np.unique(y_true)) == 2:\n",
    "        try:\n",
    "            roc = roc_auc_score(y_true, y_scores)\n",
    "        except Exception:\n",
    "            roc = None\n",
    "        try:\n",
    "            precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "            pr_auc = auc(recall, precision)\n",
    "        except Exception:\n",
    "            pr_auc = None\n",
    "\n",
    "    return dict(\n",
    "        fn=int(fn), fp=int(fp), tn=int(tn), tp=int(tp),\n",
    "        accuracy=float(acc), precision=float(pre), recall=float(rec), f1=float(f1),\n",
    "        roc_auc=(None if roc is None else float(roc)),\n",
    "        pr_auc=(None if pr_auc is None else float(pr_auc)),\n",
    "    )\n",
    "\n",
    "\n",
    "def run_model(\n",
    "    name: str,\n",
    "    estimator,\n",
    "    param_grid: Dict[str, Any],\n",
    "    Xtrain_res: pd.DataFrame,\n",
    "    Ytrain_res: pd.Series,\n",
    "    Xtrain: pd.DataFrame,\n",
    "    Ytrain: pd.Series,\n",
    "    Xtest: pd.DataFrame,\n",
    "    Ytest: pd.Series,\n",
    "    cv_splits: int = 10,\n",
    "    scoring: str = \"f1\",\n",
    "    n_jobs: int = -1,\n",
    "    save_dir: Path = Path(\".\"),\n",
    ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Grid-search a model on resampled training set, fit best params, evaluate on original\n",
    "    train and test splits, save model and CV table.\n",
    "    \"\"\"\n",
    "    print(f\"\\n===== Begin Train: {name} =====\")\n",
    "    skf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
    "    gs = GridSearchCV(estimator, param_grid, scoring=scoring, n_jobs=n_jobs, cv=skf, refit=True)\n",
    "    gs.fit(Xtrain_res, np.array(Ytrain_res).ravel())\n",
    "\n",
    "    print(f\"Best {scoring}: {gs.best_score_:.6f} using {gs.best_params_}\")\n",
    "    cv_df = pd.DataFrame(gs.cv_results_)\n",
    "    cv_path = save_dir / f\"cv_results_{name}.csv\"\n",
    "    cv_df.to_csv(cv_path, index=False)\n",
    "    print(f\"Saved CV results to: {cv_path}\")\n",
    "\n",
    "    best = gs.best_estimator_\n",
    "    # Save model\n",
    "    model_path = save_dir / f\"model_{name}.pkl\"\n",
    "    joblib.dump(best, model_path)\n",
    "    print(f\"Saved best model to: {model_path}\")\n",
    "\n",
    "    # Evaluate on original training set\n",
    "    pred_tr = best.predict(Xtrain)\n",
    "    scores_tr = get_scores(best, Xtrain)\n",
    "    metrics_tr = evaluate_split(Ytrain, pred_tr, scores_tr)\n",
    "    print(f\"[{name}] Training metrics: {metrics_tr}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    pred_te = best.predict(Xtest)\n",
    "    scores_te = get_scores(best, Xtest)\n",
    "    metrics_te = evaluate_split(Ytest, pred_te, scores_te)\n",
    "    print(f\"[{name}] Test metrics: {metrics_te}\")\n",
    "\n",
    "    return metrics_tr, metrics_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d11359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Main -------------------- #\n",
    "def main():\n",
    "    # Create output directory\n",
    "    out_dir = Path(\"model_outputs\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load data produced by your preprocessing pipeline\n",
    "    Xtrain = safe_index_col(f\"{DATA_PREFIX}-Xtrain.csv\")\n",
    "    Xtrain_ = safe_index_col(f\"{DATA_PREFIX}-Xtrain_.csv\")  # SMOTE-resampled features\n",
    "    Xtest = safe_index_col(f\"{DATA_PREFIX}-Xtest.csv\")\n",
    "\n",
    "    Ytrain = safe_index_col(f\"{DATA_PREFIX}-Ytrain.csv\").iloc[:, 0]\n",
    "    Ytrain_ = pd.read_csv(f\"{DATA_PREFIX}-Ytrain_.csv\")  # may not have index col; ensure 1D series\n",
    "    if Ytrain_.shape[1] == 1:\n",
    "        Ytrain_ = Ytrain_.iloc[:, 0]\n",
    "    else:\n",
    "        # If there are multiple columns by accident, take the first\n",
    "        Ytrain_ = Ytrain_.iloc[:, 0]\n",
    "    Ytest = safe_index_col(f\"{DATA_PREFIX}-Ytest.csv\").iloc[:, 0]\n",
    "\n",
    "    print(\"Shapes:\",\n",
    "          \"Xtrain\", Xtrain.shape, \"| Ytrain\", Ytrain.shape,\n",
    "          \"| Xtrain_\", Xtrain_.shape, \"| Ytrain_\", Ytrain_.shape,\n",
    "          \"| Xtest\", Xtest.shape, \"| Ytest\", Ytest.shape)\n",
    "\n",
    "    # Containers for aggregated results (keep your original column names)\n",
    "    out_rows = []\n",
    "\n",
    "    # ---------------- Random Forest ---------------- #\n",
    "    rf_params = dict(\n",
    "        n_estimators=[100, 200, 300, 500],\n",
    "        criterion=[\"gini\", \"entropy\", \"log_loss\"],\n",
    "        max_depth=[None, 10, 20, 50, 100],\n",
    "        min_samples_split=[2, 5, 10],\n",
    "        min_samples_leaf=[1, 2, 4],\n",
    "        n_jobs=[-1],\n",
    "        random_state=[42],\n",
    "    )\n",
    "    tr_rf, te_rf = run_model(\n",
    "        name=\"RandomForest\",\n",
    "        estimator=RandomForestClassifier(),\n",
    "        param_grid=rf_params,\n",
    "        Xtrain_res=Xtrain_, Ytrain_res=Ytrain_,\n",
    "        Xtrain=Xtrain, Ytrain=Ytrain,\n",
    "        Xtest=Xtest, Ytest=Ytest,\n",
    "        save_dir=out_dir\n",
    "    )\n",
    "    out_rows.append((\"RandomForest\", te_rf))\n",
    "\n",
    "    # ---------------- LinearSVC ---------------- #\n",
    "    # Note: LinearSVC does not support probability; we will use decision_function for AUC.\n",
    "    # Valid combos: penalty='l2', loss in {'hinge','squared_hinge'}.\n",
    "    lsvc_params = dict(\n",
    "        loss=[\"hinge\", \"squared_hinge\"],\n",
    "        C=[2.0, 1.0, 0.5, 0.2, 0.1],\n",
    "        tol=[1e-4, 1e-3],\n",
    "        max_iter=[1000, 2000, 5000],\n",
    "    )\n",
    "    tr_svc, te_svc = run_model(\n",
    "        name=\"LinearSVC\",\n",
    "        estimator=LinearSVC(dual=True),  # default dual=True works for our sizes\n",
    "        param_grid=lsvc_params,\n",
    "        Xtrain_res=Xtrain_, Ytrain_res=Ytrain_,\n",
    "        Xtrain=Xtrain, Ytrain=Ytrain,\n",
    "        Xtest=Xtest, Ytest=Ytest,\n",
    "        save_dir=out_dir\n",
    "    )\n",
    "    out_rows.append((\"LinearSVC\", te_svc))\n",
    "\n",
    "    # ---------------- Decision Tree ---------------- #\n",
    "    dt_params = dict(\n",
    "        criterion=[\"gini\", \"entropy\", \"log_loss\"],\n",
    "        splitter=[\"best\", \"random\"],\n",
    "        max_depth=[None, 10, 20, 50, 100, 200],\n",
    "        min_samples_split=[2, 5, 10],\n",
    "        min_samples_leaf=[1, 2, 4],\n",
    "        random_state=[42],\n",
    "    )\n",
    "    tr_dt, te_dt = run_model(\n",
    "        name=\"DecisionTree\",\n",
    "        estimator=DecisionTreeClassifier(),\n",
    "        param_grid=dt_params,\n",
    "        Xtrain_res=Xtrain_, Ytrain_res=Ytrain_,\n",
    "        Xtrain=Xtrain, Ytrain=Ytrain,\n",
    "        Xtest=Xtest, Ytest=Ytest,\n",
    "        save_dir=out_dir\n",
    "    )\n",
    "    out_rows.append((\"DecisionTree\", te_dt))\n",
    "\n",
    "    # ---------------- GaussianNB ---------------- #\n",
    "    # No hyperparameters to tune that matter for basic case.\n",
    "    tr_nb, te_nb = run_model(\n",
    "        name=\"GaussianNB\",\n",
    "        estimator=GaussianNB(),\n",
    "        param_grid={},  # empty grid\n",
    "        Xtrain_res=Xtrain_, Ytrain_res=Ytrain_,\n",
    "        Xtrain=Xtrain, Ytrain=Ytrain,\n",
    "        Xtest=Xtest, Ytest=Ytest,\n",
    "        save_dir=out_dir\n",
    "    )\n",
    "    out_rows.append((\"GaussianNB\", te_nb))\n",
    "\n",
    "    # ---------------- KNN ---------------- #\n",
    "    knn_params = dict(\n",
    "        n_neighbors=[3, 5, 7, 9, 11, 15, 21],\n",
    "        algorithm=[\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "        weights=[\"uniform\", \"distance\"],\n",
    "        n_jobs=[-1],\n",
    "    )\n",
    "    tr_knn, te_knn = run_model(\n",
    "        name=\"KNeighbors\",\n",
    "        estimator=KNeighborsClassifier(),\n",
    "        param_grid=knn_params,\n",
    "        Xtrain_res=Xtrain_, Ytrain_res=Ytrain_,\n",
    "        Xtrain=Xtrain, Ytrain=Ytrain,\n",
    "        Xtest=Xtest, Ytest=Ytest,\n",
    "        save_dir=out_dir\n",
    "    )\n",
    "    out_rows.append((\"KNeighbors\", te_knn))\n",
    "\n",
    "    # ---------------- Aggregate & Save ---------------- #\n",
    "    # Keep your original column order and naming\n",
    "    columns = ['fn', 'fp', 'tn', 'tp', 'accuracy', 'f1-score', 'precision', 'recall', 'roc_auc', 'pr_auc']\n",
    "    index_names = ['RandomForest', 'LinearSVC', 'DecisionTree', 'GaussianNB', 'KNeighbors']\n",
    "\n",
    "    data = []\n",
    "    for name, metrics in out_rows:\n",
    "        row = [\n",
    "            metrics['fn'], metrics['fp'], metrics['tn'], metrics['tp'],\n",
    "            metrics['accuracy'], metrics['f1'], metrics['precision'], metrics['recall'],\n",
    "            metrics['roc_auc'], metrics['pr_auc']\n",
    "        ]\n",
    "        data.append(row)\n",
    "\n",
    "    result_df = pd.DataFrame(data, index=index_names, columns=columns)\n",
    "    print(\"\\n===== Test Set Summary =====\")\n",
    "    print(result_df)\n",
    "\n",
    "    out_path = Path(\"predictive performance.csv\")\n",
    "    result_df.to_csv(out_path)\n",
    "    print(f\"\\nSaved summary to: {out_path.resolve()}\")\n",
    "\n",
    "    # Quick success guard (as in your original script)\n",
    "    if len(data) == 5:\n",
    "        print(\"success!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
