{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import (\n",
    "    max_error,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    mean_squared_log_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== BASIC CONFIG (EDIT THIS ONLY) =====================\n",
    "DATA_PREFIX = \"Event_48\"  # <<< change to your file prefix (no .csv)\n",
    "OUT_DIR = Path(\"regression_outputs\")\n",
    "RANDOM_STATE = 42\n",
    "CV_SPLITS = 10\n",
    "SCORING = \"neg_mean_squared_error\"  # primary CV objective\n",
    "# ========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Utilities -------------------- #\n",
    "def safe_index_col(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Safely read CSV with the first column as index (covers 'Unnamed: 0' cases).\n",
    "    \"\"\"\n",
    "    return pd.read_csv(path, index_col=0)\n",
    "\n",
    "\n",
    "def can_compute_msle(y_true: pd.Series, y_pred: np.ndarray) -> bool:\n",
    "    \"\"\"\n",
    "    MSLE requires non-negative targets and predictions.\n",
    "    \"\"\"\n",
    "    return (np.min(y_true) >= 0) and (np.min(y_pred) >= 0)\n",
    "\n",
    "\n",
    "def evaluate_split(y_true: pd.Series, y_pred: np.ndarray) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compute common regression metrics on a split.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"max_error\": float(max_error(y_true, y_pred)),\n",
    "        \"mean_absolute_error\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"mean_squared_error\": float(mean_squared_error(y_true, y_pred)),\n",
    "        \"r2_score\": float(r2_score(y_true, y_pred)),\n",
    "        \"mean_squared_log_error\": None,\n",
    "    }\n",
    "    if can_compute_msle(y_true, y_pred):\n",
    "        try:\n",
    "            msle = mean_squared_log_error(y_true, y_pred)\n",
    "            metrics[\"mean_squared_log_error\"] = float(msle)\n",
    "        except Exception:\n",
    "            metrics[\"mean_squared_log_error\"] = None\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def parity_plot(y_true_tr, y_pred_tr, y_true_te, y_pred_te, title: str, save_path: Path):\n",
    "    \"\"\"\n",
    "    Create side-by-side parity (y_pred vs y_true) scatter plots for train and test.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 5))\n",
    "\n",
    "    # Train\n",
    "    ax1.scatter(y_pred_tr, y_true_tr, s=30, alpha=0.35, marker='*', label=f\"MAE={mean_absolute_error(y_true_tr, y_pred_tr):.3f}\")\n",
    "    lo = min(np.min(y_true_tr), np.min(y_pred_tr))\n",
    "    hi = max(np.max(y_true_tr), np.max(y_pred_tr))\n",
    "    ax1.plot([lo, hi], [lo, hi], \"k:\", linewidth=1)\n",
    "    ax1.set_title(\"Training set\", fontsize=13)\n",
    "    ax1.set_xlabel(\"Predicted\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Measured\", fontsize=12)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Test\n",
    "    ax2.scatter(y_pred_te, y_true_te, s=30, alpha=0.35, marker='o', label=f\"MAE={mean_absolute_error(y_true_te, y_pred_te):.3f}\")\n",
    "    lo2 = min(np.min(y_true_te), np.min(y_pred_te))\n",
    "    hi2 = max(np.max(y_true_te), np.max(y_pred_te))\n",
    "    ax2.plot([lo2, hi2], [lo2, hi2], \"k:\", linewidth=1)\n",
    "    ax2.set_title(\"Test set\", fontsize=13)\n",
    "    ax2.set_xlabel(\"Predicted\", fontsize=12)\n",
    "    ax2.legend()\n",
    "\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path, dpi=180)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def run_model(\n",
    "    name: str,\n",
    "    estimator,\n",
    "    param_grid: Dict[str, Any],\n",
    "    Xtrain: pd.DataFrame,\n",
    "    Ytrain: pd.Series,\n",
    "    Xtest: pd.DataFrame,\n",
    "    Ytest: pd.Series,\n",
    "    save_dir: Path,\n",
    "    scoring: str = SCORING,\n",
    "    cv_splits: int = CV_SPLITS,\n",
    "    n_jobs: int = -1,\n",
    ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Grid-search a regressor on (Xtrain, Ytrain), fit best params, evaluate on both train and test,\n",
    "    save model, CV table and parity plots.\n",
    "    \"\"\"\n",
    "    print(f\"\\n===== Begin Train: {name} =====\")\n",
    "    kf = KFold(n_splits=cv_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    gs = GridSearchCV(estimator, param_grid, scoring=scoring, n_jobs=n_jobs, cv=kf, refit=True)\n",
    "    gs.fit(Xtrain, np.array(Ytrain).ravel())\n",
    "\n",
    "    print(f\"Best {scoring}: {gs.best_score_:.6f} using {gs.best_params_}\")\n",
    "    cv_df = pd.DataFrame(gs.cv_results_)\n",
    "    cv_path = save_dir / f\"cv_results_{name}.csv\"\n",
    "    cv_df.to_csv(cv_path, index=False)\n",
    "    print(f\"Saved CV results to: {cv_path}\")\n",
    "\n",
    "    best = gs.best_estimator_\n",
    "\n",
    "    # Save model\n",
    "    model_path = save_dir / f\"model_{name}.pkl\"\n",
    "    joblib.dump(best, model_path)\n",
    "    print(f\"Saved best model to: {model_path}\")\n",
    "\n",
    "    # Fit on full training set (already done by GridSearchCV refit=True)\n",
    "    # Predict\n",
    "    pred_tr = best.predict(Xtrain)\n",
    "    pred_te = best.predict(Xtest)\n",
    "\n",
    "    # Evaluate\n",
    "    metrics_tr = evaluate_split(Ytrain, pred_tr)\n",
    "    metrics_te = evaluate_split(Ytest, pred_te)\n",
    "    print(f\"[{name}] Training metrics: {metrics_tr}\")\n",
    "    print(f\"[{name}] Test metrics: {metrics_te}\")\n",
    "\n",
    "    # Parity plots\n",
    "    plot_path = save_dir / f\"parity_{name}.png\"\n",
    "    parity_plot(Ytrain, pred_tr, Ytest, pred_te, title=name, save_path=plot_path)\n",
    "    print(f\"Saved parity plot to: {plot_path}\")\n",
    "\n",
    "    return metrics_tr, metrics_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Main -------------------- #\n",
    "def main():\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load datasets (produced by your preprocessing pipeline)\n",
    "    Xtrain = safe_index_col(f\"{DATA_PREFIX}-Xtrain.csv\")\n",
    "    Xtrain_ = safe_index_col(f\"{DATA_PREFIX}-Xtrain_.csv\")  # available but unused for regression\n",
    "    Xtest = safe_index_col(f\"{DATA_PREFIX}-Xtest.csv\")\n",
    "\n",
    "    Ytrain = safe_index_col(f\"{DATA_PREFIX}-Ytrain.csv\").iloc[:, 0]\n",
    "    # Ytrain_ exists from your pipeline but is not used for regression\n",
    "    Ytrain_ = pd.read_csv(f\"{DATA_PREFIX}-Ytrain_.csv\")\n",
    "    if Ytrain_.shape[1] == 1:\n",
    "        Ytrain_ = Ytrain_.iloc[:, 0]\n",
    "    else:\n",
    "        Ytrain_ = Ytrain_.iloc[:, 0]\n",
    "    Ytest = safe_index_col(f\"{DATA_PREFIX}-Ytest.csv\").iloc[:, 0]\n",
    "\n",
    "    print(\"Shapes:\",\n",
    "          \"Xtrain\", Xtrain.shape, \"| Ytrain\", Ytrain.shape,\n",
    "          \"| Xtrain_\", Xtrain_.shape, \"| Ytrain_\", Ytrain_.shape,\n",
    "          \"| Xtest\", Xtest.shape, \"| Ytest\", Ytest.shape)\n",
    "\n",
    "    # Collect test metrics to aggregate at the end (keep your original column names)\n",
    "    rows = []\n",
    "\n",
    "    # ---------------- RandomForestRegressor ---------------- #\n",
    "    rf_params = dict(\n",
    "        n_estimators=[200, 300, 500],\n",
    "        criterion=[\"squared_error\", \"absolute_error\", \"friedman_mse\"],  # 'poisson' only if targets >= 0\n",
    "        max_depth=[None, 10, 20, 50, 100],\n",
    "        min_samples_split=[2, 5, 10],\n",
    "        min_samples_leaf=[1, 2, 4],\n",
    "        random_state=[RANDOM_STATE],\n",
    "        n_jobs=[-1],\n",
    "    )\n",
    "    _, te_rf = run_model(\n",
    "        name=\"RandomForestRegressor\",\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid=rf_params,\n",
    "        Xtrain=Xtrain, Ytrain=Ytrain,\n",
    "        Xtest=Xtest, Ytest=Ytest,\n",
    "        save_dir=OUT_DIR,\n",
    "    )\n",
    "    rows.append((\"RandomForestRegressor\", te_rf))\n",
    "\n",
    "    # ---------------- DecisionTreeRegressor ---------------- #\n",
    "    dt_params = dict(\n",
    "        criterion=[\"squared_error\", \"absolute_error\", \"friedman_mse\"],  # 'poisson' only if targets >= 0\n",
    "        splitter=[\"best\", \"random\"],\n",
    "        max_depth=[None, 10, 20, 50, 100, 200],\n",
    "        min_samples_split=[2, 5, 10],\n",
    "        min_samples_leaf=[1, 2, 4],\n",
    "        random_state=[RANDOM_STATE],\n",
    "    )\n",
    "    _, te_dt = run_model(\n",
    "        name=\"DecisionTreeRegressor\",\n",
    "        estimator=DecisionTreeRegressor(),\n",
    "        param_grid=dt_params,\n",
    "        Xtrain=Xtrain, Ytrain=Ytrain,\n",
    "        Xtest=Xtest, Ytest=Ytest,\n",
    "        save_dir=OUT_DIR,\n",
    "    )\n",
    "    rows.append((\"DecisionTreeRegressor\", te_dt))\n",
    "\n",
    "    # ---------------- XGBRegressor ---------------- #\n",
    "    xgb_params = dict(\n",
    "        learning_rate=[0.01, 0.05, 0.1],\n",
    "        n_estimators=[300, 500, 700, 900],\n",
    "        max_depth=[3, 5, 7, 9],\n",
    "        subsample=[0.7, 0.9, 1.0],\n",
    "        colsample_bytree=[0.7, 0.9, 1.0],\n",
    "        reg_lambda=[1.0, 5.0, 10.0],\n",
    "        random_state=[RANDOM_STATE],\n",
    "        tree_method=[\"hist\"],  # fast and robust\n",
    "    )\n",
    "    _, te_xgb = run_model(\n",
    "        name=\"XGBRegressor\",\n",
    "        estimator=xgb.XGBRegressor(),\n",
    "        param_grid=xgb_params,\n",
    "        Xtrain=Xtrain, Ytrain=Ytrain,\n",
    "        Xtest=Xtest, Ytest=Ytest,\n",
    "        save_dir=OUT_DIR,\n",
    "    )\n",
    "    rows.append((\"XGBRegressor\", te_xgb))\n",
    "\n",
    "    # ---------------- KNeighborsRegressor ---------------- #\n",
    "    knn_params = dict(\n",
    "        n_neighbors=[3, 5, 7, 11, 15, 21],\n",
    "        algorithm=[\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "        weights=[\"uniform\", \"distance\"],\n",
    "        n_jobs=[-1],\n",
    "    )\n",
    "    _, te_knn = run_model(\n",
    "        name=\"KNeighborsRegressor\",\n",
    "        estimator=KNeighborsRegressor(),\n",
    "        param_grid=knn_params,\n",
    "        Xtrain=Xtrain, Ytrain=Ytrain,\n",
    "        Xtest=Xtest, Ytest=Ytest,\n",
    "        save_dir=OUT_DIR,\n",
    "    )\n",
    "    rows.append((\"KNeighborsRegressor\", te_knn))\n",
    "\n",
    "    # ---------------- SVR ---------------- #\n",
    "    svr_params = dict(\n",
    "        kernel=[\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "        C=[0.1, 0.5, 1.0, 2.0, 5.0],\n",
    "        gamma=[\"scale\", \"auto\"],\n",
    "        degree=[2, 3],  # used only for 'poly'\n",
    "        epsilon=[0.1, 0.2, 0.5],\n",
    "        cache_size=[5000],\n",
    "        max_iter=[-1],\n",
    "    )\n",
    "    _, te_svr = run_model(\n",
    "        name=\"SVR\",\n",
    "        estimator=SVR(),\n",
    "        param_grid=svr_params,\n",
    "        Xtrain=Xtrain, Ytrain=Ytrain,\n",
    "        Xtest=Xtest, Ytest=Ytest,\n",
    "        save_dir=OUT_DIR,\n",
    "    )\n",
    "    rows.append((\"SVR\", te_svr))\n",
    "\n",
    "    # ---------------- Aggregate & Save ---------------- #\n",
    "    columns = [\"max_error\", \"mean_absolute_error\", \"mean_squared_error\", \"mean_squared_log_error\", \"r2_score\"]\n",
    "    index_names = [name for name, _ in rows]\n",
    "\n",
    "    data = []\n",
    "    for _, metrics in rows:\n",
    "        data.append([\n",
    "            metrics[\"max_error\"],\n",
    "            metrics[\"mean_absolute_error\"],\n",
    "            metrics[\"mean_squared_error\"],\n",
    "            metrics[\"mean_squared_log_error\"],\n",
    "            metrics[\"r2_score\"],\n",
    "        ])\n",
    "\n",
    "    result_df = pd.DataFrame(data, index=index_names, columns=columns)\n",
    "    print(\"\\n===== Test Set Summary (Regression) =====\")\n",
    "    print(result_df)\n",
    "\n",
    "    out_path = Path(\"predictive performance (regression).csv\")\n",
    "    result_df.to_csv(out_path)\n",
    "    print(f\"\\nSaved summary to: {out_path.resolve()}\")\n",
    "\n",
    "    if len(rows) == 5:\n",
    "        print(\"success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
